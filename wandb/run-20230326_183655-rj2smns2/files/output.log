current loss:  6.878061294555664
current loss:  7.151561737060547
current loss:  3.1689937114715576
current loss:  4.923558235168457
current loss:  6.946523666381836
current loss:  5.959934711456299
current loss:  4.819924831390381
current loss:  3.811404228210449
current loss:  5.57516622543335
current loss:  2.4853267669677734
current loss:  3.441371440887451
current loss:  3.5003223419189453
current loss:  2.2683093547821045
current loss:  3.7168667316436768
current loss:  2.806471109390259
current loss:  3.280425786972046
current loss:  2.0976195335388184
current loss:  2.4204039573669434
current loss:  2.6910505294799805
current loss:  2.010829448699951
current loss:  2.475533962249756
current loss:  1.9753432273864746
current loss:  1.6947596073150635
current loss:  2.0292701721191406
current loss:  1.4183571338653564
current loss:  1.493278980255127
current loss:  1.8796364068984985
current loss:  1.685570240020752
current loss:  1.5551996231079102
current loss:  1.3979371786117554
current loss:  1.5498629808425903
current loss:  1.588395595550537
current loss:  1.3311790227890015
current loss:  1.2870464324951172
current loss:  1.8125005960464478
current loss:  1.5067291259765625
current loss:  1.6147381067276
current loss:  1.62064790725708
current loss:  1.8718631267547607
current loss:  1.0812313556671143
current loss:  1.598336935043335
current loss:  1.2053725719451904
current loss:  1.10480797290802
current loss:  1.1270673274993896
current loss:  1.1941267251968384
current loss:  1.5688015222549438
current loss:  1.1482579708099365
current loss:  1.422698736190796
current loss:  1.2560265064239502
current loss:  1.5758119821548462
current loss:  1.123339056968689
current loss:  0.9485764503479004
current loss:  1.340548038482666
current loss:  0.8697496652603149
current loss:  0.9458707571029663
current loss:  1.2150826454162598
current loss:  0.8949077129364014
current loss:  1.8629968166351318
current loss:  0.9211328625679016
current loss:  1.16395103931427
current loss:  1.063746690750122
current loss:  1.348326325416565
current loss:  1.4400053024291992
current loss:  1.018253207206726
current loss:  0.891712486743927
current loss:  1.0784372091293335
current loss:  1.1584680080413818
current loss:  1.52565336227417
current loss:  1.1214462518692017
current loss:  1.005435824394226
current loss:  1.084928274154663
current loss:  0.8878938555717468
current loss:  0.9166077971458435
current loss:  0.9823484420776367
current loss:  0.8481373190879822
current loss:  1.4572632312774658
current loss:  1.3191618919372559
current loss:  1.264072299003601
current loss:  1.0923941135406494
current loss:  0.9416143298149109
current loss:  1.007075309753418
current loss:  0.9718499779701233
current loss:  1.0106066465377808
current loss:  0.9188296794891357
current loss:  1.1306010484695435
current loss:  0.9432362914085388
current loss:  1.0399311780929565
current loss:  1.0290383100509644
current loss:  1.0114861726760864
current loss:  0.7094040513038635
current loss:  1.760071873664856
current loss:  0.8951769471168518
current loss:  1.0546596050262451
current loss:  0.9868046641349792
current loss:  1.145372986793518
current loss:  1.0120104551315308
current loss:  0.8493607640266418
current loss:  1.5999031066894531
current loss:  1.256397008895874
current loss:  1.0995241403579712
current loss:  1.1217468976974487
current loss:  0.9155357480049133
current loss:  1.403878092765808
current loss:  1.0100970268249512
current loss:  1.1864668130874634
current loss:  1.0038793087005615
current loss:  1.0165092945098877
current loss:  0.7933987379074097
current loss:  1.076375961303711
current loss:  0.7679071426391602
current loss:  0.9291660785675049
current loss:  0.9174257516860962
current loss:  0.9776296615600586
current loss:  0.8948361277580261
current loss:  0.7551476359367371
current loss:  0.9984462857246399
current loss:  0.7694289684295654
current loss:  1.031956434249878
current loss:  1.321522831916809
current loss:  0.8357439041137695
current loss:  0.9644095301628113
current loss:  1.0390853881835938
current loss:  1.0668429136276245
current loss:  0.882755696773529
current loss:  0.9977341890335083
current loss:  0.7125351428985596
current loss:  1.056886076927185
current loss:  1.1075356006622314
current loss:  1.0689184665679932
current loss:  0.9208148121833801
current loss:  0.9137504696846008
current loss:  0.8351762294769287
current loss:  0.7455281019210815
current loss:  0.7311149835586548
current loss:  1.0774685144424438
current loss:  0.8452289700508118
current loss:  0.9934378862380981
current loss:  1.074399471282959
current loss:  0.9057114124298096
current loss:  0.9792577028274536
current loss:  0.693474292755127
current loss:  0.7890735864639282
current loss:  1.0634666681289673
current loss:  1.1744439601898193
current loss:  0.8984664678573608
current loss:  0.9196046590805054
current loss:  0.8285673260688782
current loss:  0.8229093551635742
current loss:  0.8137688636779785
current loss:  0.8759496808052063
current loss:  0.703894853591919
current loss:  1.1577092409133911
current loss:  1.0271782875061035
current loss:  0.9769636392593384
current loss:  0.7509104609489441
current loss:  1.129159688949585
current loss:  0.8800540566444397
current loss:  0.788432776927948
current loss:  0.6786490678787231
current loss:  0.7509097456932068
current loss:  1.4888261556625366
current loss:  0.8693172335624695
current loss:  1.3511508703231812
current loss:  0.9119073748588562
Traceback (most recent call last):
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/tokenizer.py", line 203, in <module>
    trainTokenizer(model, trainLoader, optimizer, criterion, device, 5, pathOrigin, True)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/tokenizer.py", line 181, in trainTokenizer
    losses[counter] = loss.detach().cpu().item()
KeyboardInterrupt
Traceback (most recent call last):
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/tokenizer.py", line 203, in <module>
    trainTokenizer(model, trainLoader, optimizer, criterion, device, 5, pathOrigin, True)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/tokenizer.py", line 181, in trainTokenizer
    losses[counter] = loss.detach().cpu().item()
