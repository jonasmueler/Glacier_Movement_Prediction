/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/TransformerSoftConditioning.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  s = layer(s)
/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([3, 1, 50, 50])) that is different to the input size (torch.Size([1, 50, 50])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Traceback (most recent call last):
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/completeTrainScriptDebug.py", line 81, in <module>
    functions.trainLoop(dTrain, model, False,"transformerPatches", 0.0001, 0.01, 0, 2, None, 1, True, device)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/functions.py", line 589, in trainLoop
    loss.backward()
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 190, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 85, in _make_grads
    raise RuntimeError("grad can be implicitly created only for scalar outputs")
RuntimeError: grad can be implicitly created only for scalar outputs
Traceback (most recent call last):
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/completeTrainScriptDebug.py", line 81, in <module>
    functions.trainLoop(dTrain, model, False,"transformerPatches", 0.0001, 0.01, 0, 2, None, 1, True, device)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/functions.py", line 589, in trainLoop
    loss.backward()
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 190, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 85, in _make_grads
    raise RuntimeError("grad can be implicitly created only for scalar outputs")
RuntimeError: grad can be implicitly created only for scalar outputs
tensor([[[0.2648, 0.2996, 0.2402,  ..., 0.2575, 0.2399, 0.2756],
         [0.2507, 0.2976, 0.3176,  ..., 0.2994, 0.3124, 0.3021],
         [0.2700, 0.3750, 0.2280,  ..., 0.2977, 0.2546, 0.2270],
         ...,
         [0.2704, 0.2607, 0.3119,  ..., 0.3361, 0.3158, 0.2727],
         [0.3876, 0.5491, 0.2664,  ..., 0.3091, 0.3333, 0.2746],
         [0.2950, 0.2331, 0.2732,  ..., 0.2323, 0.2459, 0.3142]]],
       grad_fn=<AddBackward0>)