/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([10, 50, 50])) that is different to the input size (torch.Size([10, 1, 50, 50])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
current loss:  11.556051254272461
current loss:  9.356996536254883
current loss:  8.17186450958252
current loss:  13.037182807922363
current loss:  13.716824531555176
current loss:  12.189423561096191
current loss:  10.89614486694336
current loss:  9.342653274536133
current loss:  6.94954252243042
current loss:  13.837462425231934
current loss:  9.870409965515137
current loss:  13.607643127441406
current loss:  13.920198440551758
current loss:  13.90051555633545
current loss:  11.064680099487305
current loss:  8.538446426391602
current loss:  9.595091819763184
current loss:  13.958539009094238
current loss:  10.400152206420898
current loss:  7.137016296386719
current loss:  13.310863494873047
current loss:  17.269380569458008
current loss:  12.383468627929688
current loss:  16.091339111328125
current loss:  11.761520385742188
current loss:  12.536120414733887
current loss:  8.586344718933105
current loss:  8.171137809753418
current loss:  14.212035179138184
current loss:  12.266566276550293
current loss:  8.01060676574707
current loss:  9.298823356628418
current loss:  11.199179649353027
current loss:  11.4268217086792
current loss:  12.182024002075195
current loss:  11.697944641113281
current loss:  7.237739562988281
current loss:  17.094484329223633
current loss:  10.863373756408691
current loss:  11.221762657165527
current loss:  8.095206260681152
current loss:  12.538293838500977
current loss:  9.646102905273438
current loss:  8.170842170715332
current loss:  17.841426849365234
current loss:  9.285235404968262
current loss:  11.999449729919434
current loss:  7.694295406341553
current loss:  9.817325592041016
current loss:  13.08119010925293
current loss:  8.897553443908691
current loss:  8.42206859588623
current loss:  16.285442352294922
current loss:  9.364130973815918
current loss:  8.290799140930176
current loss:  7.991996765136719
current loss:  7.776087760925293
current loss:  12.573454856872559
current loss:  7.86497688293457
current loss:  10.863475799560547
current loss:  5.47139310836792
current loss:  13.140352249145508
current loss:  10.272811889648438
current loss:  13.909607887268066
current loss:  10.16004753112793
current loss:  6.422648906707764
current loss:  11.156898498535156
current loss:  9.946911811828613
current loss:  5.169899940490723
current loss:  8.56554126739502
current loss:  6.4290452003479
current loss:  8.230649948120117
current loss:  13.640274047851562
current loss:  12.7916841506958
current loss:  7.967676639556885
current loss:  10.955312728881836
current loss:  6.385342121124268
current loss:  12.724174499511719
current loss:  10.07083797454834
current loss:  6.875164985656738
current loss:  13.534377098083496
current loss:  12.987601280212402
current loss:  21.897539138793945
current loss:  9.856181144714355
current loss:  12.703509330749512
current loss:  14.47594165802002
current loss:  7.575437068939209
current loss:  7.756202697753906
current loss:  13.245343208312988
current loss:  11.87609577178955
current loss:  9.269709587097168
current loss:  15.882203102111816
current loss:  12.343764305114746
current loss:  11.898958206176758
current loss:  9.1900053024292
current loss:  4.654400825500488
current loss:  10.140894889831543
current loss:  7.059154987335205
current loss:  6.343937873840332
current loss:  9.031193733215332
current loss:  9.948822021484375
current loss:  8.68294906616211
current loss:  11.9153413772583
current loss:  5.8062238693237305
current loss:  9.617142677307129
current loss:  11.237414360046387
current loss:  7.270340442657471
current loss:  12.906311988830566
current loss:  9.905094146728516
current loss:  9.782994270324707
current loss:  6.850858688354492
current loss:  12.239989280700684
current loss:  11.089433670043945
current loss:  9.949174880981445
current loss:  5.111636638641357
current loss:  6.500699520111084
current loss:  9.39264965057373
current loss:  7.735668182373047
current loss:  7.528207778930664
current loss:  8.71934700012207
current loss:  11.519850730895996
current loss:  8.837218284606934
current loss:  13.892465591430664
current loss:  11.19799518585205
current loss:  9.28577995300293
current loss:  9.622750282287598
current loss:  11.05911636352539
current loss:  7.683993339538574
current loss:  6.105584144592285
current loss:  8.708150863647461
current loss:  9.045570373535156
current loss:  6.200734615325928
current loss:  10.400619506835938
current loss:  9.968490600585938
current loss:  6.9055495262146
current loss:  10.566351890563965
current loss:  13.066431045532227
current loss:  10.349668502807617
current loss:  15.297378540039062
current loss:  7.90236759185791
current loss:  9.404682159423828
current loss:  4.263274192810059
current loss:  10.81924057006836
current loss:  7.924337863922119
current loss:  9.510512351989746
current loss:  10.938389778137207
current loss:  9.87773609161377
current loss:  7.267458438873291
current loss:  6.488310813903809
current loss:  7.549912929534912
current loss:  8.39001750946045
current loss:  9.300198554992676
current loss:  6.29744291305542
current loss:  2.6777713298797607
current loss:  6.734985828399658
current loss:  10.871768951416016
current loss:  10.338695526123047
current loss:  6.711523056030273
current loss:  10.00680160522461
current loss:  5.333625793457031
current loss:  10.368764877319336
current loss:  10.693014144897461
current loss:  7.393756866455078
current loss:  6.550438404083252
current loss:  13.558242797851562
current loss:  10.652090072631836
current loss:  9.343673706054688
current loss:  7.26409912109375
current loss:  8.915885925292969
current loss:  8.044079780578613
current loss:  5.4830780029296875
current loss:  4.1441168785095215
current loss:  9.390243530273438
current loss:  6.66523551940918
current loss:  11.586085319519043
current loss:  10.502755165100098
current loss:  7.007302284240723
current loss:  7.639629364013672
current loss:  11.393444061279297
current loss:  6.561172008514404
current loss:  6.686153888702393
current loss:  13.354426383972168
current loss:  8.085259437561035
current loss:  5.280185222625732
current loss:  6.762484073638916
current loss:  10.248401641845703
current loss:  9.134427070617676
current loss:  8.549324035644531
current loss:  8.451231002807617
current loss:  9.929719924926758
current loss:  3.560274600982666
current loss:  4.928239822387695
current loss:  7.401474952697754
current loss:  4.867039203643799
Traceback (most recent call last):
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/tokenizer.py", line 179, in <module>
    trainTokenizer(model, trainLoader, optimizer, criterion, device, 5, pathOrigin, True)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/tokenizer.py", line 153, in trainTokenizer
    loss.backward()
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/tokenizer.py", line 179, in <module>
    trainTokenizer(model, trainLoader, optimizer, criterion, device, 5, pathOrigin, True)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/tokenizer.py", line 153, in trainTokenizer
    loss.backward()
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
current loss:  6.344052791595459
current loss:  9.900670051574707
current loss:  7.357644557952881
current loss:  6.10172700881958
current loss:  10.165863037109375
current loss:  6.726644992828369
current loss:  4.8017048835754395
