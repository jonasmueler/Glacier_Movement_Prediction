
torch.Size([100, 4, 1600])
epoch:  0 , example:  1  current loss =  1.2458301782608032
torch.Size([100, 4, 1600])
epoch:  0 , example:  2  current loss =  1.2431060075759888
torch.Size([100, 4, 1600])
Traceback (most recent call last):
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/transformerTrain.py", line 45, in <module>
    functions.trainLoop(dataTrain, dataVal, Tokenizer,  model, loss, False, "TransformerMiddle", params, True, device)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/functions.py", line 594, in trainLoop
    forward = model.forward(inpts, targets, training = True)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/transformerBase.py", line 133, in forward
    l = self.latentSpace(x, y, training)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/transformerBase.py", line 97, in latentSpace
    out = self.transformer(flattenedInput, targets, tgt_mask=targetMask)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 147, in forward
    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 333, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 652, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 669, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1167, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/functional.py", line 5046, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/functional.py", line 4745, in _in_projection_packed
    return (linear(q, w_q, b_q),) + linear(k, w_kv, b_kv).chunk(2, dim=-1)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/fx/traceback.py", line 51, in format_stack
    @compatibility(is_backward_compatible=False)
KeyboardInterrupt
Traceback (most recent call last):
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/transformerTrain.py", line 45, in <module>
    functions.trainLoop(dataTrain, dataVal, Tokenizer,  model, loss, False, "TransformerMiddle", params, True, device)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/functions.py", line 594, in trainLoop
    forward = model.forward(inpts, targets, training = True)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/transformerBase.py", line 133, in forward
    l = self.latentSpace(x, y, training)
  File "/media/jonas/B41ED7D91ED792AA/Arbeit_und_Studium/Kognitionswissenschaft/Semester_5/masterarbeit#/data_Code/code/transformerBase.py", line 97, in latentSpace
    out = self.transformer(flattenedInput, targets, tgt_mask=targetMask)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 147, in forward
    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 333, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 652, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 669, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1167, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/functional.py", line 5046, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/nn/functional.py", line 4745, in _in_projection_packed
    return (linear(q, w_q, b_q),) + linear(k, w_kv, b_kv).chunk(2, dim=-1)
  File "/home/jonas/.local/lib/python3.10/site-packages/torch/fx/traceback.py", line 51, in format_stack
    @compatibility(is_backward_compatible=False)
